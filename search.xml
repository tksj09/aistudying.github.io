<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[服务器性能测试]]></title>
    <url>%2F2018%2F11%2F01%2Fserver-performance-test%2F</url>
    <content type="text"><![CDATA[1 概述1.1背景本文的编写背景是目前机房服务器资源存在未充分使用的现象，为了合理分 配资源，现需要对服务器自身性能进行评估，探索一套评估方法，从而为后续资源合理分配提供依据。 1.2评测指标简单来说，服务器硬件性能指标来自于测试对象，一般x86服务器的主要组 成有CPU、内存、硬盘、网卡等。针对单机，评测指标重点关注CPU、内存、IO、网络；对于集群，重点关注网络、高可用。 本文主要评测单机性能，指标如下： CPU—计算能力 内存—延时、速率 IO—读写能力 网络—网络带宽 1.3工具概况CPU主流评测工具有Linux平台的SPECCPU、linpack，Windows平台的Sandra。 内存主流评测工具有Linux平台的stream，Windows平台的Sandra。 IO主流评测工具有Linux平台的Iozone，Windows平台的Iometer。 网络主流评测工具有Linux平台的iperf、netperf。 其他一些对整体系统进行评测的工具就不再介绍了，例如SPEC系列、TPC基准系列。 2 CPU性能2.1评测方法关于Linux测试cpu性能，有3个重要的概念：上下文切换（context switchs），运行队列（Run queue）和使用率（utilization）。 业务运行中最关注的CPU项就是使用率，使用率是和业务负载强相关的，通常可以通过监控软件或Linux系统工具获取。 另一个基本评测值就是计算能力，包括整数、浮点计算，可以使用SPECCPU、linpack测试。 2.2评测工具CPU使用率是平时最关注的性能项，采用系统工具或第三方工具都可以。具体工具不再详述。 CPU整数、浮点计算能力，是服务器发布必测项，但是对于客户却不太重要，客户关注的是业务运行时CPU的能力是否会成为瓶颈。评测工具主要有SPECCPU2006、Linpack： SPEC CPU 2006包括了CINT2006和C FP2006两个子项目，前者用于测量和对比整数性能，而后者则用于测量和对比浮点性能，SPEC CPU 2006包括了12项整数运算和17项浮点运算。 Linpack现在在国际上已经成为最流行的用于测试高性能计算机系统浮点性能的benchmark。通过利用高性能计算机，用高斯消元法求解N元一次稠密线性代数方程组的测试，评价高性能计算机的浮点性能，测试结果以浮点运算每秒（Flops）给出。 3 内存性能3.1评测方法内存性能一般关注的指标是延时、带宽，测试方法有Windows下使用Sandra，Linux使用stream进行评测，可以得到当前内存的实际速率、延时。这是单独针对内存的测试方法，但在实际应用中通常对内存的评测是在压力、稳定性、性能测试时的监测内存的使用。 3.2评测工具STREAM是业界广为流行的综合性内存带宽实际性能测量工具之一。随着处理器处理核心数量的增多，内存带宽对于提升整个系统性能越发重要，如果某个系统不能够足够迅速地将内存中的数据传输到处理器当中，若干处理核心就会处于等待数据的闲置状态，而这其中所产生的闲置时间不仅会降低系统的效率还会抵消多核心和高主频所带来的性能提升因素。STREAM具有良好的空间局部性，是对TLB友好、Cache友好的一款测试。STREAM支持Copy 、Scale 、 Add、 Triad四种操作。 4 IO性能4.1评测方法服务器的存储性能也就是指IO性能，通常评测的重点是各种数据块(512B、4K、2M…)下的读写能力，具体指标有IOPS、带宽、时延。测试原理是工具对存储加压也就是产生各种读写操作来测试整个IO的最大能力。 4.2评测工具Iometer是Windows系统下对存储子系统的读写性能进行测试的软件。可以显示磁盘系统的最大IO能力、磁盘系统的最大吞吐量、CPU使用率、错误信息等。用户可以通过设置不同的测试的参数，有存取类型(如sequential ,random)、读写块大小(如64K、256K)，队列深度等，来模拟实际应用的读写环境进行测试。 IOzone主要用来测试操作系统文件系统性能的测试工具，该工具所测试的范围主要有，write , Re-write, Read, Re-Read, Random Read, Random Write, Random Mix, Backwards Read, Record Rewrite, Strided Read, Fwrite, Frewrite, Fread, Freread, Mmap, Async I/O。使用iozone可以在多线程、多cpu，并指定cpu cache空间大小以及同步或异步I/O读写模式的情况下进行测试文件操作性能。 5 网络性能5.1评测方法 网络带宽不难理解，就是系统收发包时的最大流量，一般分为TCP、UDP两种模式。影响服务器网络性能主要是网卡的性能，其他如系统内核、驱动也是相关因素。测试方法一般需要2台服务器进行收发包，一端server，一端client。 5.2评测工具Iperf是一个网络性能测试工具。Iperf可以测试TCP和UDP带宽质量。Iperf可以测量最大TCP带宽，具有多种参数和UDP特性。Iperf可以报告带宽，延迟抖动和数据包丢失。利用Iperf这一特性，可以用来测试一些网络设备如路由器，防火墙，交换机等的性能。 Netperf是一种网络性能的测量工具，主要针对基于TCP或UDP的传输。 Netperf根据应用的不同，可以进行不同模式的网络性能测试，即批量数据传输（bulk data transfer）模式和请求/应答（request/reponse）模式。Netperf测试结果所反映的是一个系统能够以多快的速度向另外一个系统发送数据，以及另外一个系统能够以多快的速度接收数据。]]></content>
      <categories>
        <category>linux</category>
        <category>server</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>performance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04安装go1.9.2]]></title>
    <url>%2F2018%2F11%2F01%2Fubuntu16.04-install-go1.9.2%2F</url>
    <content type="text"><![CDATA[安装步骤1234curl -O https://www.golangtc.com/static/go/1.9.2/go1.9.2.linux-amd64.tar.gz tar -C /usr/local -zxvf go1.9.2.linux-amd64.tar.gz vim /etc/profile 打开/etc/profile后 最后一行插入123export GOROOT=/usr/local/go export PATH=$PATH:$GOROOT/bin export GOPATH=/home/daniel/go 然后 source /etc/profile GOROOT：go语言的安装路径GOPATH：go语言中跟工作空间相关的环境变量，这个变量指定go语言的工作空间位置，是将所有的代码保存在一个工作区中，具体是指一个目录 生成开发目录12345cd /home/daniel/gomkdir binmkdir srcmkdir pkg 之后构建go项目放在src下面， 生成的安装包会自动放在bin下，生成过程中的中间文件会放在pkg下面 常用包获取12345go get github.com/astaxie/beegogo get github.com/go-sql-driver/mysqlgo get github.com/eclipse/paho.mqtt.golanggo get gopkg.in/mgo.v2go get github.com/beego/bee 默认会下载到GOPATH的src目录下如果使用go get下载较慢，可以使用其他方式将项目代码下载并拷贝到GOPATH的src目录下，然后切换到项目源码下进行编译]]></content>
      <categories>
        <category>linux</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>software</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github Page和hexo创建博客全记录]]></title>
    <url>%2F2018%2F10%2F02%2Fbuilding-blog-with-hexo-and-githubpage%2F</url>
    <content type="text"><![CDATA[安装git安装Node.js见notes的另一篇文章 安装hexo创建GithubPage创建hexo站点hexo 分支管理恢复环境123git clone -b hexo https://github.com/aistudying/aistudying.github.io hexocd hexonpm install 提交hexo分支修改，删除模块等多余文件123git add .git commit -m "source blog"git push 配置next主题1、基本信息配置 基本信息包括：博客标题、作者、描述、语言等等。 打开 站点配置文件 ，找到Site模块123456title: 标题subtitle: 副标题description: 描述author: 作者language: 语言（简体中文是zh-Hans, 英文是en）timezone: 网站时区（Hexo 默认使用您电脑的时区，不用写） 关于 站点配置文件 中的其他配置可参考站点配置 2、菜单设置 菜单包括：首页、归档、分类、标签、关于等等 我们刚开始默认的菜单只有首页和归档两个，不能够满足我们的要求，所以需要添加菜单，打开主题配置文件找到Menu Settings 123456789menu: home: / || home //首页 archives: /archives/ || archive //归档 categories: /categories/ || th //分类 tags: /tags/ || tags //标签 about: /about/ || user //关于 #schedule: /schedule/ || calendar //日程表 #sitemap: /sitemap.xml || sitemap //站点地图 #commonweal: /404/ || heartbeat //公益404 看看你需要哪个菜单就把哪个取消注释打开就行了；关于后面的格式，以archives: /archives/ || archive为例：|| 之前的/archives/表示标题“归档”，关于标题的格式可以去themes/next/languages/zh-Hans.yml中参考或修改|| 之后的archive表示图标，可以去Font Awesome中查看或修改，Next主题所有的图标都来自Font Awesome。 3、Next主题样式设置我们百里挑一选择了Next主题，不过Next主题还有4种风格供我们选择，打开主题配置文件找到Scheme Settings12345# Schemes# scheme: Muse# scheme: Mist# scheme: Piscesscheme: Gemini 4种风格大同小异，本人用的是Gemini风格，你们可以选择自己喜欢的风格。 4、侧栏设置 侧栏设置包括：侧栏位置、侧栏显示与否、文章间距、返回顶部按钮等等 打开主题配置文件找到sidebar字段12345678910111213141516sidebar:# Sidebar Position - 侧栏位置（只对Pisces | Gemini两种风格有效） position: left //靠左放置 #position: right //靠右放置# Sidebar Display - 侧栏显示时机（只对Muse | Mist两种风格有效） #display: post //默认行为，在文章页面（拥有目录列表）时显示 display: always //在所有页面中都显示 #display: hide //在所有页面中都隐藏（可以手动展开） #display: remove //完全移除 offset: 12 //文章间距（只对Pisces | Gemini两种风格有效） b2t: false //返回顶部按钮（只对Pisces | Gemini两种风格有效） scrollpercent: true //返回顶部按钮的百分比 5、头像设置打开主题配置文件找到Sidebar Avatar字段12# Sidebar Avataravatar: /images/header.jpg 这是头像的路径，只需把你的头像命名为header.jpg（随便命名）放入themes/next/source/images中，将avatar的路径名改成你的头像名就OK啦！ 6、设置RSS(未验证)1、先安装 hexo-generator-feed 插件1$ npm install hexo-generator-feed --save 2、打开站点配置文件找到Extensions在下面添加123456789# RSS订阅feed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: ' ' 3、打开 主题配置文件 找到rss，设置为1rss: /atom.xml 7、添加分类模块1、新建一个分类页面1$ hexo new page categories 2、你会发现你的source文件夹下有了categorcies/index.md，打开index.md文件将title设置为title: 分类3、打开主题配置文件找到menu，将categorcies取消注释，分类模块即添加完成 4、写文章时只需在文章的顶部标题下方添加categories字段，即可自动创建分类名并加入对应的分类中举个栗子：12title: 分类测试文章标题categories: 分类名 8、添加标签模块1、新建一个标签页面1$ hexo new page tags 2、你会发现你的source文件夹下有了tags/index.md，打开index.md文件将title设置为title: 标签3、打开主题配置文件找到menu，将tags取消注释，标签模块即添加完成 4、写文章时只需在文章的顶部标题下方添加tags字段，即可自动创建标签名并归入对应的标签中举个栗子：1234title: 标签测试文章标题tags: - 标签1 - 标签2 9、添加关于模块1、新建一个关于页面1$ hexo new page about 2、你会发现你的source文件夹下有了about/index.md，打开index.md文件即可编辑关于你的信息，可以随便编辑。3、打开 主题配置文件 找到menu，将about取消注释 10、添加搜索功能1、安装 hexo-generator-searchdb 插件1$ npm install hexo-generator-searchdb --save 2、打开站点配置文件找到Extensions在下面添加123456# 搜索search: path: search.xml field: post format: html limit: 10000 3、打开 主题配置文件 找到Local search，将enable设置为true]]></content>
      <categories>
        <category>web</category>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F10%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>web</category>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04安装最新版本Node.js]]></title>
    <url>%2F2018%2F09%2F27%2Fubuntu-install-latest-nodejs%2F</url>
    <content type="text"><![CDATA[ubuntu自带的 node.js 版本太低安装最新版本node.js需要两步即可第一步，去 nodejs 官网 https://nodejs.org 看最新的版本号官网提供两个版本，一个是lts版本，长期支持，较稳定，一个是最新版本，有最新特性，但是可能存在未知bug第二步，根据版本号，更新软件源并安装node.js 的每个大版本号都有相对应的源，比如这里的 8.x.x版本的源是https://deb.nodesource.com/setup_8.x我这里安装LTS版，版本号为8.12.0123sudo apt-get install curl python-software-propertiescurl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -sudo apt-get install -y nodejs 查看版本号：1234~# npm -v6.4.1~# node -vv8.12.0]]></content>
      <categories>
        <category>linux</category>
        <category>software</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>software</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04使用kubeadm部署k8s]]></title>
    <url>%2F2018%2F09%2F20%2Fdeploy-k8s-with-kubeadm-ubuntu16.04%2F</url>
    <content type="text"><![CDATA[关闭防火墙(所有节点)123456systemctl stop ufwsystemctl disable ufwswapoff -a //将swapoff -a命令写入/etc/rc.loacl文件中，开机执行vim /etc/fstab注释swap分区所在行 启用路由转发123456net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-arptables = 1net.ipv4.ip_forward = 1iptables -P FORWARD ACCEPT 安装Docker(所有节点)注：由于后面想试用k8s-device-plugin特性来管理GPU，所以需要使用nvidia-docker2，而nvidia-docker2要求docker-ce版本最低为18.06，所以要安装docker-ce18.06版本 1234567方法1：curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun方法2：curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;apt-get -y updateapt-get -y install docker-ce=18.06.1~ce~3-0~ubuntu 安装nvidia-docker2(所有节点)12345678curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \ sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \ sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get -y updateapt-get install -y nvidia-docker2pkill -SIGHUP dockerd 配置k8s源(所有节点)1234567891011121314151617181920Ubuntu16.04:deb http://mirrors.aliyun.com/ubuntu/ xenial main restricteddeb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricteddeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiversedeb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial mainCentOS7:cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装kubeadm(所有节点)#安装依赖 apt-get install -y apt-transport-https socat aufs-tools cgroupfs-mount libltdl7 pigz apt-get install cri-tools=1.11.0-00 \ kubeadm=1.11.1-00 \ kubectl=1.11.1-00 \ kubelet=1.11.1-00 \ kubernetes-cni=0.6.0-00 导入镜像(主节点和计算节点均执行，但导入内容不一样)1234567891011121314151617主节点：k8s.gcr.io_coredns:1.1.3k8s.gcr.io_etcd-amd64:3.2.18k8s.gcr.io_kube-apiserver-amd64:v1.11.1k8s.gcr.io_kube-controller-manager-amd64:v1.11.1k8s.gcr.io_kube-proxy-amd64:v1.11.1k8s.gcr.io_kube-scheduler-amd64:v1.11.1k8s.gcr.io_pause:3.1quay.io_calico_cni:v3.1.3quay.io_calico_node:v3.1.3子节点：k8s.gcr.io_kube-proxy-amd64:v1.11.1k8s.gcr.io_kubernetes-dashboard-amd64:v1.8.3k8s.gcr.io_pause__3.1:da86e6ba6ca1quay.io_calico_cni:v3.1.3quay.io_calico_node:3.1.3 初始化主节点(主节点)12345678910#init主节点kubeadm init --pod-network-cidr=172.16.0.0/16 --kubernetes-version=v1.11.1 | tee k8s.init.log##使用flannel时网络要修改为10.244.0.0/16，示例如下kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=v1.11.1 | tee k8s.init.log#上面这步一定要记录好日志，尤其是节点加入的命令，因为后续无法重现，格式类似下面kubeadm join 192.168.1.109:6443 --token t7fra0.dhjys8zaka71zo80 --discovery-token-ca-cert-hash sha256:4344ddef08a385abf37300693708806f62024eb6f77ce3994dc523d4b4dbe346mkdir -p $HOME/.kubecp /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/config 部署网络flannel(主节点)123kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml 加入k8s集群1kubeadm join 192.168.1.109:6443 --token t7fra0.dhjys8zaka71zo80 --discovery-token-ca-cert-hash sha256:4344ddef08a385abf37300693708806f62024eb6f77ce3994dc523d4b4dbe346 查看集群状态12345678910111213141516171819# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster Ready master 5m v1.11.1node02 Ready &lt;none&gt; 15s v1.11.1# kubectl get pod --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEkube-system coredns-78fcdf6894-5pjm7 1/1 Running 0 6m 10.244.0.4 masterkube-system coredns-78fcdf6894-vjh89 1/1 Running 0 6m 10.244.0.5 masterkube-system etcd-master 1/1 Running 0 5m 192.168.1.109 masterkube-system kube-apiserver-master 1/1 Running 0 5m 192.168.1.109 masterkube-system kube-controller-manager-master 1/1 Running 0 5m 192.168.1.109 masterkube-system kube-flannel-ds-amd64-42lhb 1/1 Running 0 1m 192.168.1.251 node02kube-system kube-flannel-ds-amd64-tn5lf 1/1 Running 0 2m 192.168.1.109 masterkube-system kube-proxy-9zq7l 1/1 Running 0 6m 192.168.1.109 masterkube-system kube-proxy-cvhsr 1/1 Running 0 1m 192.168.1.251 node02kube-system kube-scheduler-master 1/1 Running 0 5m 192.168.1.109 master 简单测试123456789kubectl run -i --tty busybox --image=busybox --restart=Never -- sh测试DNS解析kubectl run -ti --rm test --image=busybox:1.28.4 --restart=Never -- nslookup baidu.com# 正常执行，再看看调度情况$ kubectl get pod --show-all -o wideNAME READY STATUS RESTARTS AGE IP NODEbusybox 0/1 Completed 0 48s 10.244.1.2 node1 部署k8s-device-plugin12345678910111213#所有节点修改/etc/docker/daemon.json文件内容如下 &#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [] &#125; &#125;&#125;# 在主节点上部署k8s-device-plugin服务$ kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.11/nvidia-device-plugin.yml 部署dashboard1kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 登录dashboard根据官方介绍一共有三种方式：NodePort方式、kubectl proxy 方式、API server方式 kubectl proxy方式： 1234master节点上执行kubectl proxy --address=&apos;0.0.0.0&apos; --accept-hosts=&apos;^*$&apos;浏览器访问：http://192.168.207.129:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ NodePort方式： 修改kubernetes-dashboard.yaml文件中最后的部分service，再重新部署 需要注意的是nodeport方式使用https方式连接，但是谷歌浏览器对安全验证无法通过，使用火狐浏览器在高级里面添加例外即可访问。 12345678910111213spec: # 添加Service的type为NodePort type: NodePort ports: - port: 443 targetPort: 8443 # 添加映射到虚拟机的端口,只支持30000--32767的端口 nodePort: 30001 selector: k8s-app: kubernetes-dashboard浏览器访问：https：//master ip:nodeport #如https://192.168.207.129:30001 用户登录验证 有两种方式：kube-config和token，这里我们使用token方式就可以了，因为kube-config文件中实际上包含用户名和token，因此token的获取是必须的 1234获取token：在master节点上执行kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token然后在浏览器输入获取的token即可登录 部署Heapster、InfluxDB和Grafana12345678910111213141516171819202122232425262728293031wget https://github.com/kubernetes/heapster/archive/v1.5.4.tar.gztar -zxvf v1.5.4.tar.gz &amp;&amp; cd heapster-1.5.4/deploy/kube-config/ll influxdb/#-rw-rw-r-- 1 root root 2290 7月 26 21:24 grafana.yaml#-rw-rw-r-- 1 root root 1114 7月 26 21:24 heapster.yaml#-rw-rw-r-- 1 root root 974 7月 26 21:24 influxdb.yamlll rbac/#-rw-rw-r-- 1 root root 264 7月 26 21:24 heapster-rbac.yaml以上4个yaml文件即为需要用到的文件所需用到的镜像：gcr.io/google_containers/heapster-amd64:v1.5.3gcr.io/google_containers/heapster-grafana-amd64:v4.4.3gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3修改 grafana.yaml文件spec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port type: NodePort # 去掉注释即可 ports: - port: 80 targetPort: 3000 selector: k8s-app: grafana修改为nodport类型，将Grafana暴露在宿主机Node的端口上，以便后续浏览器访问 grafana 的 admin UI 界面开始执行部署：kubectl create -f influxdb/ &amp;&amp; kubectl create -f rbac/ 遇到的问题：failed to get all container stats from Kubelet URL 所有节点执行以下操作即可： 12345678910vi /etc/systemd/system/kubelet.service.d/10-kubeadm.confchange to like thisEnvironment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --read-only-port=10255 &quot;thensystemctl daemon-reloadsystemctl restart kubelet 删除节点12345主节点：kubectl drain node02 --delete-local-data --force --ignore-daemonsetskubectl delete node node02计算节点：kubeadm reset 常用命令1234567891011121314151617181920kubectl get pod --all-namespaces -o widekubectl apply -f kubernetes-dashboard.yamlkubectl delete -f kubernetes-dashboard.yamlkubectl -n kube-system get svc kubernetes-dashboardkubectl get pods --namespace kube-system查看可用GPU资源kubectl get nodes &quot;-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\.com/gpu&quot;kubeadm init失败清理命令kubeadm resetifconfig cni0 downip link delete cni0ifconfig flannel.1 downip link delete flannel.1rm -rf /var/lib/cni/使master node参与工作负载kubectl taint nodes --all node-role.kubernetes.io/master- 忘记初始master节点时的node节点加入集群命令怎么办123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 解决coredns无限重启问题12345mkdir /run/systemd/resolve/vim /run/systemd/resolve/resolv.conf添加真实的DNS服务器vim /var/lib/kubelet/config.yaml修改resolve路径 在k8s运行tensorflow分布式在集群中任意一台物理机上执行以下操作 1234567891011121314151617181920212223242526git clone https://github.com/tensorflow/tensorflow.gitcd tensorflow/tensorflow/tools/dist_test生成k8s部署集群的yaml文件scripts/k8s_tensorflow.py \ --num_workers 2 \ --num_parameter_servers 2 \ --grpc_port 2222 \ --request_load_balancer true \ --docker_image &quot;tensorflow/tf_grpc_server&quot; \ &gt; tf-k8s-with-lb.yaml#生成后,执行部署kubectl create -f tf-k8s-with-lb.yaml#构建客户端测试镜像并测试./remote_test.sh... // 镜像打包输出NUM_WORKERS = 2NUM_PARAMETER_SERVERS = 2SETUP_CLUSTER_ONLY = 0GRPC_SERVER_URLS: SYNC_REPLICAS: 0GRPC port to be used when creating the k8s TensorFlow cluster: 2222Path to gcloud binary: /var/gcloud/google-cloud-sdk/bin/gcloud gcloud service account key file cannot be found at: /var/gcloud/secrets/tensorflow-testing.json // 这里并不重要FAILED to determine GRPC server URLs of all workers // 错误提示,并且上面GRPC_SERVER_URLS:也为空 根据错误提示，执行以下命令 1234567docker run -it tensorflow/tf-dist-test-client // 此为刚才remote_test.sh生成的镜像,docker hub上并没有进入容器内export TF_DIST_GRPC_SERVER_URLS=&quot;grpc://10.244.0.16:2222 grpc://10.244.1.31:2222&quot;cd /var/tf-dist-test/scripts./dist_test.sh或者执行下面命令亦可./dist_mnist_test.sh &quot;grpc://10.244.0.16:2222 grpc://10.244.1.31:2222&quot; --num-workers 2 --num-parameter-servers 1]]></content>
      <categories>
        <category>k8s</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16安装caffe]]></title>
    <url>%2F2018%2F05%2F24%2Fubuntu16-install-caffe%2F</url>
    <content type="text"><![CDATA[安装驱动准备工作 禁用nouveau 12$sudo -s$echo "blacklist nouveau"&gt;&gt;/etc/modprobe.d/blacklist.conf 重新生成initrd文件 12$sudo mv /boot/initrd.img-$(uname -r) /boot/initrd.img-$(uname -r)-nouveau$sudo update-initramfs -u 编辑/etc/default/grub 让ubuntu显示引导信息，方便以后调试维护1$sudo vi /etc/default/grub 将 GRUB_CMDLINE_LINUX_DEFAULT=里的quiet去掉，splash改为nosplash然后sudo update-grub重启后可以用命令lsmod| grep nouveau验证是否正确屏蔽了nouveau运行sudo /etc/init.d/lightdm stop，然后使用ctrl+alt+F1进入tty1 安装驱动这里可以选择用独立驱动（版本不低于381.09）进行安装.独立驱动：安装NVIDIA-Linux-x86_64-381.09.run（若是集显，则加上参数–no-opengl-files)依次按提示做出选择即可 安装CUDACUDA安装包内置显卡驱动，但是一般版本较低，为满足该版本CUDA的最低驱动版本，一般推荐独立安装显卡驱动1sudo ./cuda_8.0.61_linux.run toolkit安装在默认路径：/usr/local/cuda-8.0Samples安装在~/NVIDIA_CUDA-8.0_Samples/ 添加环境变量12echo 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt;~/.bashrcecho 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' &gt;&gt;~/.bashrc 然后source~/.bashrc使环境变量生效 安装cudnn5.1123cp cudnn-8.0-linux-x64-v5.1.tgz ~cd /usr/localsudo tar zxvf ~/cudnn-8.0-linux-x64-v5.1.tgz 注：由于未知的原因，在装好以上系统部分的软件后，有可能重启后无法进入系统，表现为左上角一直有个光标，系统完全锁死。通过ssh登录后，dmesg发现内核已经崩溃，解决方法如下：123sudo dpkg-reconfigure lightdm //重新配置lightdmsudo update-initramfs -u //重新生成initrd文件sudo apt-get install --reinstall lightdm //重新安装lightdm 安装caffe 手动安装caffe（官方版本，此版本对多卡支持不好，可安装NV优化过的caffe） 12sudo apt-get install net-toolssudo apt-get install python-software-properties 安装依赖 1sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler libatlas-base-dev python-pip python-dev python-numpy python-scipy python-skimage python-matplotlib python-h5py python-leveldb python-networkx python-pandas python-dateutil python-protobuf python-gflags python-yaml libboost-filesystem-dev libboost-thread-dev gfortran cython python-pil libboost-dev libopenblas-dev libboost-all-dev 安装PyCaffe依赖项 12cd caffepip install -r python/requirements.txt 安装nccl从 https://github.com/NVIDIA/nccl 下载nccl 123unzip nccl-master.zipcd nccl-mastersudo make PREFIX=/usr/local/cuda install nccl默认安装在/usr/local/cuda/lib，可以手动复制到/usr/local/cuda/lib641sudo cp -ar /usr/local/cuda/lib/* /usr/local/cuda/lib64 安装nvcaffe从 https://github.com/NVIDIA/caffe 下载nv优化过的caffe后解压 123unzip caffe-caffe-0.15.zipcd caffe-caffe-0.15cp Makefile.config.example Makefile.config 修改Makefile.config（共3处） 123a) #USE_CUDNN := 1修改为：USE_CUDNN := 1 b) #USE_NCCL := 1修改为：USE_NCCL := 1 c) #WITH_PYTHON_LAYER := 1修改为：WITH_PYTHON_LAYER := 1 编译 12make all -j 12make pycaffe 最后添加PyCaffe的环境变量 123vim ~/.bashrc #打开 export PYTHONPATH=/home/usrname/caffe/python:$PYTHONPATH #每个人都不一样，根据caffe所在路径填写source ~/.bashrc #生效 之后再终端上输入python， 然后输入import caffe，便可以知道是否相连接成功。如无报错，则为配置成功。 相关错误及解决办法 错误1：fatal error: hdf5.h: No such file or directory解决办法：sudo cp -ar /usr/include/hdf5/serial/* /usr/include 错误2：/usr/bin/ld: cannot find -lhdf5_hl /usr/bin/ld: cannot find -lhdf5解决办法：sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial_hl.so /usr/lib/libhdf5_hl.so sudo ln -s /usr/lib/x86_64-linux-gnu/libhdf5_serial.so /usr/lib/libhdf5.so 测试caffe切换到caffe根目录准备测试数据：1sh data/mnist/get_mnist.sh 这个地方下载很慢，可以手动下载后放到data/mnist目录12345cd data/mnistwget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gzwget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gzwget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gzwget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz 然后手动运行解压命令：1gunzip *.gz 创建测试数据,回到caffe主目录：12cd ../../sh examples/mnist/create_mnist.sh 测试：1time sh examples/mnist/train_lenet.sh 测试多卡：修改examples/mnist/train_lenet.sh，在后边加上--gpu 0,1,2,…7]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>caffe</tag>
        <tag>cuda</tag>
        <tag>nvidia</tag>
      </tags>
  </entry>
</search>
